{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a78267",
   "metadata": {},
   "source": [
    "# Creating a word cloud from documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by extracting all of the documentation text into a single large text file. \n",
    "\n",
    "For each piece of text, we use an NLP library to tokenise and lemmatise it (trying to ensure that multiple variants of the same word are treated as the same).\n",
    "\n",
    "Note that you should clone the [Causal Testing Framework](https://github.com/CITCOM-project/CausalTestingFramework) repository and replace the path in the cell below with the path to the documentation directory in your local copy of the repository."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8583bd8939852835"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc68395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Ignoring non-UTF-8 file: {file_path}\")\n",
    "        return \"\"\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    lemmatized_text = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            text = extract_text_from_file(file_path)\n",
    "            if text:\n",
    "                lemmatized_text.append(tokenize_and_lemmatize(text))\n",
    "    return '\\n\\n'.join(lemmatized_text)\n",
    "\n",
    "                    \n",
    "# Replace 'your_directory_path' with the path to your documentation!\n",
    "directory_text = process_directory('CausalTestingFramework/docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the WordCloud package to process our large text file into a graphical word cloud. \n",
    "We have added some stop words -- words that should not be included in the word count -- such as 'doc' and 'github'\n",
    ", which are more related to the documentation itself rather than the program that is being documented."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81ddbe4f19779eca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "wc = WordCloud(background_color='white', \n",
    "     stopwords = nltk_stopwords.union(set(['doc','documentation','github','sphinx','http'])), width = 1000, height = 1000).generate(directory_text)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9ab4a",
   "metadata": {},
   "source": [
    "# A superficial analysis of module dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b4208",
   "metadata": {},
   "source": [
    "Pydeps (https://github.com/thebjorn/pydeps) creates a dependency graph by scanning import statements \n",
    "in python files.\n",
    "\n",
    "To execute the cell below and render the generated DOT source code, you must install `dot`.\n",
    "Follow this page https://www.graphviz.org/download/ to install `dot` on your system.\n",
    "On Windows, you can follow the [installation procedure](https://forum.graphviz.org/t/new-simplified-installation-procedure-on-windows/224). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pydeps \"CausalTestingFramework/causal_testing\" -Tpng --noshow --pylib-all --include-missing --cluster --rankdir LR"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the cell above has been executed, you should see a file called `causal_testing.png` in the current directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f5c0e1cc0a27db8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
